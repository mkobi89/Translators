Operationalisierung der Übersetzungen

Läubli

Automatische Metriken (BLEU) --> Ähnlichkeitsfunktionen, Überlappungen von 1-, 2-, 3- und 4-Wort-Sequenzen (Präzision) und bestraft kurze Kandidaten mit einer Brevity Penalty (um Unvollständigkeit zu bestrafen).  Varianzphänomene (Synonymie) --> mehreren Referenzübersetzungen 

Humane Evaluation: Verlässliche Qualitätsurteile, Adäquatheit und Flüssigkeit, «continuous» Likert-Skalen (Graham et al., 2013).


Wintis:
Keine Metrik wie BLEU anwenden --> für die Evaluation von MT-Output entwickelt wurde. 

Humane Evaluation:
Referenzübersetzung erstellen (Winti, eine Übersetzer*in erstellt, zwei Übersetzer*innen geprüft und überarbeitet) --> in kleinere Segmente (Teilsätze) zerlegen --> unvollständige Sätze nicht verlieren.
Alignierung der erstellten Zieltexte (ELF und EdE) mit Referenztextsegmenten.

1. Bewertungsrunde --> Fluency --> deutschen Segmente (monolingual evaluation of target text segment), einzeln präsentieren
2. Bewertungsrunde --> Accuracy --> jedes deutschen Segment mit zugehörigem englischen Segment des Ausgangstextes (bilingual evaluation of target text segment).

- Bewerter*innen: Amazon Mechanical Turk, Guidelines for Academic Requesters, Läubli? nur Deutsch auf hohem Niveau, mit deutscher Referenzübersetzung vergleichen
- Bewertung auf einer Strecke per Cursor anstelle 5-Punkte-Likert-Skala? --> Programmieren
- Einverständniserklärung deckt externe Bewertung ab?




Lieber Matthias

Danke für dein Mail. Ja, meine Ferien waren recht erholsam, danke.

Maureen und ich haben uns nochmals ausführlich mit deinen Vorschlägen beschäftigt und wir bleiben dabei: Wir raten dir von BLEU ab und sind der Meinung, wenn du die Qualität der produzierten Textteile beurteilen willst, kommst du um eine Humanbewertung nicht herum.

BLEU wurde entwickelt, um den Output von maschinellen Übersetzungssystemen irgendwie beurteilen zu können. Das heisst, es sagt etwas über die Qualität der Engine aus und dies in Relation zu meistens nur einer Humanübersetzung, über deren Erstellerin und Qualität man nichts weiss. Es nützen dir auch mehrere Referenzübersetzungen nichts, weil BLEU nicht erfassen kann, ob der Textinhalt stimmt. Zudem müsstest du ja dann für die Humanbewertungen ebenfalls alle produzierten Referenzübersetzungen verwenden, was den Aufwand für die Bewerterinnen vervielfachen würde.

Unser Vorgehen zur Erstellung einer Referenzübersetzung ist die Umsetzung von Good Practice in der Qualitätssicherung. Es kann nicht dazu gebraucht werden, mehrere Referenzübersetzungen zu basteln. Wenn du Zusätzliche willst, musst du dich an eine Übersetzungsdienstleisterin wenden. Maura wird mit der Erstellung je einer Referenzübersetzung nicht vor dem 4. Mai beginnen, da sie bis dann Ferien hat.

Wir empfehlen dir, die Textsegmente von «Social Info» und «Energy Advisors» nicht miteinander zu mischen. Aber die Segmente desselben Textthemas schon, unabhängig ob sie aus der ELF- und EdE-Version entstanden sind. Wir empfehlen dir auch, sie randomisiert und einzeln zu präsentieren – aber das ist wohl nichts Neues für dich. Wie gesagt, würden wir in der 1. Runde nur in Bezug auf Fluency und in der 2. Runde nur auf Accuracy im Vergleich zum jeweiligen Referenztextsegment beurteilen lassen. Es scheint uns praktikabel, dass PraktikantInnen mit sehr guten Deutschkenntnissen die Beurteilung bewerkstelligen.

Ich schicke dir eine grosse Welle Motivation und grüsse dich herzlich!

Andrea